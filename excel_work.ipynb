{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9aGn8cqX0UR-"
      },
      "source": [
        "#code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "TslrBItLV7Xu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "from datetime import datetime\n",
        "\n",
        "import openpyxl\n",
        "\n",
        "\n",
        "def find_sheets_with_values(file_path, values_to_check):\n",
        "    xls = pd.ExcelFile(file_path, engine='openpyxl')\n",
        "\n",
        "    # List to hold names of sheets that contain any of the specified values\n",
        "    sheets_with_values = []\n",
        "\n",
        "    # Iterate through each sheet in the Excel file\n",
        "    for sheet_name in xls.sheet_names:\n",
        "        # Read the current sheet into a DataFrame\n",
        "        df = pd.read_excel(xls, sheet_name=sheet_name)\n",
        "\n",
        "        # Check if any of the specified values are in the DataFrame\n",
        "        if df.isin(values_to_check).any().any():  # Checks all cells in the DataFrame\n",
        "            sheets_with_values.append(sheet_name)\n",
        "\n",
        "    return sheets_with_values\n",
        "\n",
        "\n",
        "def load_sheet_to_df(file_path, sheet_name):\n",
        "    # Load a specific worksheet into a DataFrame\n",
        "    df = pd.read_excel(file_path, sheet_name=sheet_name, engine='openpyxl')\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# Function to find the first occurrence of any keyword\n",
        "def find_first_keyword(df, keywords):\n",
        "    # Iterate over each row by its index\n",
        "    for index, row in df.iterrows():\n",
        "        # Check each cell in the row\n",
        "        for col_index, (col_name, cell) in enumerate(row.items()):\n",
        "            if cell in keywords:\n",
        "                return index, col_name, col_index\n",
        "    return None, None, None  # Return None if no keyword is found\n",
        "\n",
        "\n",
        "\n",
        "# Function to extract all data from a specific starting point in the DataFrame\n",
        "def extract_subset_df(df, start_row_index, start_col_index):\n",
        "    # Slicing the DataFrame to get all rows and columns from the specified starting indices\n",
        "    new_df = df.iloc[start_row_index:, start_col_index:]\n",
        "    return new_df\n",
        "\n",
        "def extract_first_nine_columns(df):\n",
        "    # Slice to select the first 9 columns\n",
        "    new_df = df.iloc[:, :9]  # \"9\" means up to but not including the 10th column\n",
        "    return new_df\n",
        "\n",
        "def remove_all_nan_rows(df):\n",
        "    # Drop rows where all elements are NaN\n",
        "    df_cleaned = df.dropna(how='all')\n",
        "    return df_cleaned\n",
        "\n",
        "\n",
        "\n",
        "# # Function to split the time range and format it, handling NaN values\n",
        "# def split_and_format_time(df, column):\n",
        "#     # Split the 'Time Range' into two separate columns\n",
        "#     df[['From Time', 'To Time']] = df[column].str.split('-', expand=True)\n",
        "\n",
        "#     # Format the 'From Time' and 'To Time' columns, adding handling for NaN values\n",
        "#     df['SHIFT FROM'] = df['From Time'].apply(lambda x: f\"{x[:2]}:{x[2:]}\" if pd.notna(x) else np.nan)\n",
        "#     df['SHIFT TO'] = df['To Time'].apply(lambda x: f\"{x[:2]}:{x[2:]}\" if pd.notna(x) else np.nan)\n",
        "\n",
        "#     return df\n",
        "\n",
        "# import pandas as pd\n",
        "# import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def convert_date_column(date_item):\n",
        "    # Check if the item is already a datetime object\n",
        "    if isinstance(date_item, datetime):\n",
        "        return date_item.strftime('%Y-%m-%d')  # Format and return\n",
        "\n",
        "    # If it's a string, check if it's already in 'YYYY-MM-DD' format\n",
        "    try:\n",
        "        if isinstance(date_item, str) and datetime.strptime(date_item, '%Y-%m-%d'):\n",
        "            return date_item  # Return the date as is\n",
        "    except ValueError:\n",
        "        pass  # Not in 'YYYY-MM-DD' format, so proceed to parse and convert\n",
        "\n",
        "    try:\n",
        "        # Parse the date string into datetime\n",
        "        dt = parser.parse(date_item)\n",
        "        # Return formatted date\n",
        "        return dt.strftime('%Y-%m-%d')\n",
        "    except (ValueError, TypeError):\n",
        "        # Return None or some default value if the date is invalid\n",
        "        return None\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "SFqL80eVatR4"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "\n",
        "\n",
        "\n",
        "def split_and_format_time(df, column):\n",
        "    # Initialize the 'data_check' column\n",
        "    df['swift_check'] = np.where(df[column].isna(), 'Check', df[column])\n",
        "\n",
        "    # Check if the value is properly formatted as '0000-0000'\n",
        "    def check_format(value):\n",
        "        if pd.isna(value):\n",
        "            return 'Check'\n",
        "        parts = value.split('-')\n",
        "        if len(parts) != 2 or any(len(part) != 4 for part in parts):\n",
        "            return 'Check'\n",
        "        return value  # Return the original value if it's correctly formatted\n",
        "\n",
        "    df['swift_check'] = df[column].apply(check_format)\n",
        "\n",
        "    # Split the 'Time Range' into two separate columns\n",
        "    df[['From Time', 'To Time']] = df[column].str.split('-', expand=True)\n",
        "\n",
        "    # Format the 'From Time' and 'To Time' columns, adding handling for NaN values\n",
        "    df['SHIFT FROM'] = df['From Time'].apply(lambda x: f\"{x[:2]}:{x[2:]}\" if pd.notna(x) else np.nan)\n",
        "    df['SHIFT TO'] = df['To Time'].apply(lambda x: f\"{x[:2]}:{x[2:]}\" if pd.notna(x) else np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "def format_and_check_dates(df, column_name):\n",
        "      # Create a new column for the checks\n",
        "      df['Data_check'] = 'Check'  # Default to 'Check'\n",
        "\n",
        "      # Function to try and parse dates\n",
        "      def parse_date(date):\n",
        "          try:\n",
        "              # Attempt to parse the date\n",
        "              parsed_date = parser.parse(str(date), dayfirst=True)\n",
        "              # Successfully parsed, format to 'YYYY-MM-DD'\n",
        "              return parsed_date.strftime('%Y-%m-%d')\n",
        "          except ValueError:\n",
        "              # Parsing failed, return 'Check'\n",
        "              return 'Check'\n",
        "\n",
        "      # Apply the function to the specified column and store results in a new column\n",
        "      df['Formatted_date'] = df[column_name].apply(parse_date)\n",
        "\n",
        "      # Update 'Data_check' where dates are correctly parsed\n",
        "      df['Data_check'] = df['Formatted_date'].apply(lambda x: 'OK' if x != 'Check' else 'Check')\n",
        "\n",
        "      return df\n",
        "\n",
        "\n",
        "\n",
        "def verify_product(df):\n",
        "      # Calculate the product of HOURS and RATE\n",
        "      df['Calculated_COST'] = df['HOURS_FINAL'] * df['RATE_FINAL']\n",
        "\n",
        "      # Initialize the check column with 'OK' where the calculated product matches COST and handle NaNs\n",
        "      df['HOURS_RATE_COST_check'] = np.where(\n",
        "          (df['Calculated_COST'] == df['COST_FINAL']) &\n",
        "          df['HOURS_FINAL'].notna() & df['RATE_FINAL'].notna() & df['COST_FINAL'].notna(),\n",
        "          'OK',\n",
        "          'Check'\n",
        "      )\n",
        "\n",
        "      # Optionally, drop the 'Calculated_COST' column if it's not needed\n",
        "      df.drop(columns=['Calculated_COST'], inplace=True)\n",
        "\n",
        "      return df\n",
        "\n",
        "\n",
        "def clean_and_check_cost(df, column_name):\n",
        "      # Clean the column by removing any non-numeric characters except the decimal point\n",
        "      df['COST_Cleaned_Price'] = df[column_name].replace('[^\\d.]', '', regex=True)\n",
        "\n",
        "      # Convert the cleaned strings to numeric type (float)\n",
        "      df['COST_FINAL'] = pd.to_numeric(df['COST_Cleaned_Price'], errors='coerce')\n",
        "\n",
        "      # Create a new column to indicate the validity of the data\n",
        "      df['COST_Data_check'] = 'OK'  # Default to 'OK'\n",
        "      df.loc[df['COST_FINAL'].isna(), 'COST_Data_check'] = 'Check'  # Mark non-convertible or missing entries\n",
        "\n",
        "      return df\n",
        "\n",
        "\n",
        "def clean_and_check_rate(df, column_name):\n",
        "      # Clean the column by removing any non-numeric characters except the decimal point\n",
        "      df['RATE_Cleaned_Price'] = df[column_name].replace('[^\\d.]', '', regex=True)\n",
        "\n",
        "      # Convert the cleaned strings to numeric type (float)\n",
        "      df['RATE_FINAL'] = pd.to_numeric(df['RATE_Cleaned_Price'], errors='coerce')\n",
        "\n",
        "      # Create a new column to indicate the validity of the data\n",
        "      df['RATE_Data_check'] = 'OK'  # Default to 'OK'\n",
        "      df.loc[df['RATE_FINAL'].isna(), 'RATE_Data_check'] = 'Check'  # Mark non-convertible or missing entries\n",
        "\n",
        "      return df\n",
        "\n",
        "def clean_and_check_hours(df, column_name):\n",
        "      # Clean the column by removing any non-numeric characters except the decimal point\n",
        "      df['HOURS_Cleaned_Price'] = df[column_name].replace('[^\\d.]', '', regex=True)\n",
        "\n",
        "      # Convert the cleaned strings to numeric type (float)\n",
        "      df['HOURS_FINAL'] = pd.to_numeric(df['HOURS_Cleaned_Price'], errors='coerce')\n",
        "\n",
        "      # Create a new column to indicate the validity of the data\n",
        "      df['HOURS_Data_check'] = 'OK'  # Default to 'OK'\n",
        "      df.loc[df['HOURS_FINAL'].isna(), 'HOURS_Data_check'] = 'Check'  # Mark non-convertible or missing entries\n",
        "\n",
        "      return df\n",
        "\n",
        "\n",
        "def standardize_status_values(df, column_name):\n",
        "      # Define the allowed values\n",
        "      allowed_values = ['VACANT', 'NEW', 'PENDING','CURRENT']\n",
        "\n",
        "      # Standardize the entries in the column\n",
        "      df[\"STATUS_check\"] = df[column_name].str.upper().str.strip()\n",
        "\n",
        "      # Replace values not matching the allowed values with NaN or another placeholder\n",
        "      df[\"STATUS_check\"] = df[column_name].apply(lambda x: x if x in allowed_values else 'CHECK')\n",
        "\n",
        "      return df\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xXt0_q69S-PD"
      },
      "outputs": [],
      "source": [
        "file_path=\"/content/data/HNELHD NON-SPECIALIST LOCUM VACANCY LIST (17) (1).xlsx\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "SWCF7745TEAs"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '/content/data/HNELHD NON-SPECIALIST LOCUM VACANCY LIST (17) (1).xlsx'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[9], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m values_to_check \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVACANT\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPENDING\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNEW\u001b[39m\u001b[38;5;124m'\u001b[39m,\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCURRENT\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m----> 2\u001b[0m sheets_found \u001b[38;5;241m=\u001b[39m \u001b[43mfind_sheets_with_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalues_to_check\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m##any form of  ['VACANT', 'PENDING', 'NEW','CURRENT'] check\u001b[39;00m\n",
            "Cell \u001b[0;32mIn[6], line 10\u001b[0m, in \u001b[0;36mfind_sheets_with_values\u001b[0;34m(file_path, values_to_check)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfind_sheets_with_values\u001b[39m(file_path, values_to_check):\n\u001b[0;32m---> 10\u001b[0m     xls \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# List to hold names of sheets that contain any of the specified values\u001b[39;00m\n\u001b[1;32m     13\u001b[0m     sheets_with_values \u001b[38;5;241m=\u001b[39m []\n",
            "File \u001b[0;32m~/anaconda3/envs/three-eleven/lib/python3.11/site-packages/pandas/io/excel/_base.py:1567\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[0;34m(self, path_or_buffer, engine, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[1;32m   1565\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[0;32m-> 1567\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1568\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1569\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1570\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1571\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/three-eleven/lib/python3.11/site-packages/pandas/io/excel/_openpyxl.py:553\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03mReader using openpyxl engine.\u001b[39;00m\n\u001b[1;32m    543\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;124;03m    Arbitrary keyword arguments passed to excel engine.\u001b[39;00m\n\u001b[1;32m    551\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    552\u001b[0m import_optional_dependency(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mopenpyxl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 553\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    554\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    555\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    556\u001b[0m \u001b[43m    \u001b[49m\u001b[43mengine_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    557\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/anaconda3/envs/three-eleven/lib/python3.11/site-packages/pandas/io/excel/_base.py:563\u001b[0m, in \u001b[0;36mBaseExcelReader.__init__\u001b[0;34m(self, filepath_or_buffer, storage_options, engine_kwargs)\u001b[0m\n\u001b[1;32m    559\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m IOHandles(\n\u001b[1;32m    560\u001b[0m     handle\u001b[38;5;241m=\u001b[39mfilepath_or_buffer, compression\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmethod\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28;01mNone\u001b[39;00m}\n\u001b[1;32m    561\u001b[0m )\n\u001b[1;32m    562\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(filepath_or_buffer, (ExcelFile, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workbook_class)):\n\u001b[0;32m--> 563\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    564\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[1;32m    565\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    567\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workbook_class):\n\u001b[1;32m    568\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbook \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
            "File \u001b[0;32m~/anaconda3/envs/three-eleven/lib/python3.11/site-packages/pandas/io/common.py:882\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[1;32m    874\u001b[0m             handle,\n\u001b[1;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    879\u001b[0m         )\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m--> 882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    883\u001b[0m     handles\u001b[38;5;241m.\u001b[39mappend(handle)\n\u001b[1;32m    885\u001b[0m \u001b[38;5;66;03m# Convert BytesIO or file objects passed with an encoding\u001b[39;00m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/data/HNELHD NON-SPECIALIST LOCUM VACANCY LIST (17) (1).xlsx'"
          ]
        }
      ],
      "source": [
        "values_to_check = ['VACANT', 'PENDING', 'NEW','CURRENT']\n",
        "sheets_found = find_sheets_with_values(file_path, values_to_check)\n",
        "\n",
        "##any form of  ['VACANT', 'PENDING', 'NEW','CURRENT'] check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H8WkwOakTEaJ",
        "outputId": "dc42ea78-df80-4793-a60d-36f8e7d9605c"
      },
      "outputs": [],
      "source": [
        "sheets_found"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 720
        },
        "id": "8sgMpztCk7cR",
        "outputId": "4e2a0fd6-b80f-46af-8406-4907ffe74280"
      },
      "outputs": [],
      "source": [
        "data_frame = load_sheet_to_df(file_path, \"ARMIDALE CMO ED\")\n",
        "data_frame"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Dx-_xltlamUJ",
        "outputId": "f7db1182-e978-4318-d7c5-2a0ca0666cdb"
      },
      "outputs": [],
      "source": [
        "main_df=empty_df = pd.DataFrame(columns=[\"STATUS\", \"DATE\", \"SHIFT\", \"HOURS\", \"RATE\", \"COST\", \"ON_CALL\", \"ROLE\",\"AREA\"])\n",
        "\n",
        "\n",
        "for i in sheets_found:\n",
        "  try:\n",
        "    data_frame = load_sheet_to_df(file_path, i)\n",
        "    keywords = ['VACANT', 'PENDING', 'NEW','CURRENT']\n",
        "    row_index, col_name, col_index = find_first_keyword(data_frame, keywords)\n",
        "    row_index = row_index\n",
        "    col_index = col_index\n",
        "    new_df = extract_subset_df(data_frame, row_index, col_index)\n",
        "\n",
        "\n",
        "    new_df_2 = new_df.dropna(axis=1, how='all')\n",
        "    threshold = len(new_df_2.columns) / 2  # More than half the number of columns\n",
        "    new_df_3 = new_df_2.dropna(thresh=threshold, axis=0)\n",
        "    threshold = len(new_df_3) / 2  # More than half the number of rows\n",
        "    new_df_4 = new_df_3.dropna(thresh=threshold, axis=1)\n",
        "\n",
        "    new_column_names = [\"STATUS\", \"DATE\", \"SHIFT\", \"HOURS\", \"RATE\", \"COST\", \"ON_CALL\", \"ROLE\",\"AREA\"]\n",
        "\n",
        "    if len(new_column_names) == len(new_df_4.columns):\n",
        "        new_df_4.columns = new_column_names\n",
        "    else:\n",
        "        print(\"Error: The number of new column names does not match the number of existing columns.\")\n",
        "\n",
        "    df_5=split_and_format_time(new_df_4,\"SHIFT\")\n",
        "    df_6 = format_and_check_dates(df_5, 'DATE')\n",
        "    df_7 = standardize_status_values(df_6, 'STATUS')\n",
        "    df_8 = clean_and_check_hours(df_7, 'HOURS')\n",
        "    df_9 = clean_and_check_rate(df_8, 'RATE')\n",
        "    df_10 = clean_and_check_cost(df_9, 'COST')\n",
        "    df_11 = verify_product(df_10)\n",
        "\n",
        "    main_df = pd.concat([main_df, df_11], axis=0)\n",
        "    print(\"sheet_done: \",i)\n",
        "\n",
        "  except Exception as e:\n",
        "    print(e,\":\",i)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 860
        },
        "id": "M1TFlq-QmzJA",
        "outputId": "0d789b96-2e35-45d7-c89d-ecaff10845a6"
      },
      "outputs": [],
      "source": [
        "main_df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "VQChthqxatPv",
        "outputId": "af860782-2223-4568-ae87-c19235042a1b"
      },
      "outputs": [],
      "source": [
        "{   }\n",
        "{   }\n",
        "{   }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "v1_hcNyQTTeI",
        "outputId": "428c8d24-2b4b-4aab-eb15-c1f7d3e03468"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "data_frame = load_sheet_to_df(file_path, \"ARMIDALE O&G\")\n",
        "keywords = ['VACANT', 'PENDING', 'NEW','CURRENT']\n",
        "row_index, col_name, col_index = find_first_keyword(data_frame, keywords)\n",
        "row_index = row_index\n",
        "col_index = col_index\n",
        "new_df = extract_subset_df(data_frame, row_index, col_index)\n",
        "new_df\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "mDXGPHOBTTcA",
        "outputId": "03645a1f-8271-4e90-c5af-f04a78c95a2b"
      },
      "outputs": [],
      "source": [
        "new_df_2 = new_df.dropna(axis=1, how='all')\n",
        "threshold = len(new_df_2.columns) / 2  # More than half the number of columns\n",
        "new_df_3 = new_df_2.dropna(thresh=threshold, axis=0)\n",
        "threshold = len(new_df_3) / 2  # More than half the number of rows\n",
        "new_df_4 = new_df_3.dropna(thresh=threshold, axis=1)\n",
        "new_df_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 339
        },
        "id": "eWo0zcABT89M",
        "outputId": "8a3aa11e-6b23-460e-be4c-06ef66eb4efe"
      },
      "outputs": [],
      "source": [
        "new_column_names = [\"STATUS\", \"DATE\", \"SHIFT\", \"HOURS\", \"RATE\", \"COST\", \"ON_CALL\", \"ROLE\", \"AREA\"]\n",
        "\n",
        "# Check if the length of new column names matches the number of columns in the DataFrame\n",
        "if len(new_column_names) == len(new_df_4.columns):\n",
        "    new_df_4.columns = new_column_names\n",
        "else:\n",
        "    print(f\"Error: Expected {len(new_df_4.columns)} column names but got {len(new_column_names)}.\")\n",
        "    # Optionally, handle different cases:\n",
        "    if len(new_column_names) < len(new_df_4.columns):\n",
        "        # Add a placeholder for missing column names\n",
        "        new_column_names += ['UNNAMED_' + str(i) for i in range(1, len(new_df_4.columns) - len(new_column_names) + 1)]\n",
        "        new_df_4.columns = new_column_names\n",
        "    else:\n",
        "        # Truncate the list of new column names to match the DataFrame's column count\n",
        "        new_df_4.columns = new_column_names[:len(new_df_4.columns)]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 833
        },
        "id": "ZyjdiXZnjAcU",
        "outputId": "20aea26f-fe9e-43a4-b54f-4678df292788"
      },
      "outputs": [],
      "source": [
        "new_df_4"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Q-q72iYzjGtR",
        "outputId": "3d25de77-e8a1-435e-f2d7-a2478fd4abc4"
      },
      "outputs": [],
      "source": [
        "def split_and_format_time(df, column):\n",
        "    # Initialize the 'data_check' column\n",
        "    df['swift_check'] = np.where(df[column].isna(), 'Check', df[column])\n",
        "\n",
        "    # Check if the value is properly formatted as '0000-0000'\n",
        "    def check_format(value):\n",
        "        if pd.isna(value):\n",
        "            return 'Check'\n",
        "        parts = value.split('-')\n",
        "        if len(parts) != 2 or any(len(part) != 4 for part in parts):\n",
        "            return 'Check'\n",
        "        return value  # Return the original value if it's correctly formatted\n",
        "\n",
        "    df['swift_check'] = df[column].apply(check_format)\n",
        "\n",
        "    # Split the 'Time Range' into two separate columns\n",
        "    df[['From Time', 'To Time']] = df[column].str.split('-', expand=True)\n",
        "\n",
        "    # Format the 'From Time' and 'To Time' columns, adding handling for NaN values\n",
        "    df['SHIFT FROM'] = df['From Time'].apply(lambda x: f\"{x[:2]}:{x[2:]}\" if pd.notna(x) else np.nan)\n",
        "    df['SHIFT TO'] = df['To Time'].apply(lambda x: f\"{x[:2]}:{x[2:]}\" if pd.notna(x) else np.nan)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "df_5=split_and_format_time(new_df_4,\"SHIFT\")\n",
        "df_5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_PNUUi-FjGqq",
        "outputId": "199254b8-a476-4f9e-f58e-f7a9bbcbe8cd"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from dateutil import parser\n",
        "\n",
        "def format_and_check_dates(df, column_name):\n",
        "    # Create a new column for the checks\n",
        "    df['Data_check'] = 'Check'  # Default to 'Check'\n",
        "\n",
        "    # Function to try and parse dates\n",
        "    def parse_date(date):\n",
        "        try:\n",
        "            # Attempt to parse the date\n",
        "            parsed_date = parser.parse(str(date), dayfirst=True)\n",
        "            # Successfully parsed, format to 'YYYY-MM-DD'\n",
        "            return parsed_date.strftime('%Y-%m-%d')\n",
        "        except ValueError:\n",
        "            # Parsing failed, return 'Check'\n",
        "            return 'Check'\n",
        "\n",
        "    # Apply the function to the specified column and store results in a new column\n",
        "    df['Formatted_date'] = df[column_name].apply(parse_date)\n",
        "\n",
        "    # Update 'Data_check' where dates are correctly parsed\n",
        "    df['Data_check'] = df['Formatted_date'].apply(lambda x: 'OK' if x != 'Check' else 'Check')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Apply the function\n",
        "df_6 = format_and_check_dates(df_5, 'DATE')\n",
        "df_6"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "hI_SH8ZXl4uW",
        "outputId": "829f4f54-31c9-4b7b-a08e-8a217509725e"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def standardize_status_values(df, column_name):\n",
        "    # Define the allowed values\n",
        "    allowed_values = ['VACANT', 'NEW', 'PENDING','CURRENT']\n",
        "\n",
        "    # Standardize the entries in the column\n",
        "    df[\"STATUS_check\"] = df[column_name].str.upper().str.strip()\n",
        "\n",
        "    # Replace values not matching the allowed values with NaN or another placeholder\n",
        "    df[\"STATUS_check\"] = df[column_name].apply(lambda x: x if x in allowed_values else 'CHECK')\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Apply the function\n",
        "df_7 = standardize_status_values(df_6, 'STATUS')\n",
        "df_7\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lVnrblfUl4r0",
        "outputId": "c6d52785-9ecc-4627-e682-0c93cdc6f691"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_and_check_prices(df, column_name):\n",
        "    # Clean the column by removing any non-numeric characters except the decimal point\n",
        "    df['HOURS_Cleaned_Price'] = df[column_name].replace('[^\\d.]', '', regex=True)\n",
        "\n",
        "    # Convert the cleaned strings to numeric type (float)\n",
        "    df['HOURS_FINAL'] = pd.to_numeric(df['HOURS_Cleaned_Price'], errors='coerce')\n",
        "\n",
        "    # Create a new column to indicate the validity of the data\n",
        "    df['HOURS_Data_check'] = 'OK'  # Default to 'OK'\n",
        "    df.loc[df['HOURS_FINAL'].isna(), 'HOURS_Data_check'] = 'Check'  # Mark non-convertible or missing entries\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function\n",
        "df_8 = clean_and_check_prices(df_7, 'HOURS')\n",
        "df_8\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PAvTHgdXoha0",
        "outputId": "b786514d-f4e1-4a46-b832-b51b989ec4bf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_and_check_prices(df, column_name):\n",
        "    # Clean the column by removing any non-numeric characters except the decimal point\n",
        "    df['RATE_Cleaned_Price'] = df[column_name].replace('[^\\d.]', '', regex=True)\n",
        "\n",
        "    # Convert the cleaned strings to numeric type (float)\n",
        "    df['RATE_FINAL'] = pd.to_numeric(df['RATE_Cleaned_Price'], errors='coerce')\n",
        "\n",
        "    # Create a new column to indicate the validity of the data\n",
        "    df['RATE_Data_check'] = 'OK'  # Default to 'OK'\n",
        "    df.loc[df['RATE_FINAL'].isna(), 'RATE_Data_check'] = 'Check'  # Mark non-convertible or missing entries\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function\n",
        "df_9 = clean_and_check_prices(df_8, 'RATE')\n",
        "df_9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "rcAm5_rSohYb",
        "outputId": "b9f2dbf5-9112-485c-958f-9de6fc7eb76f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "def clean_and_check_prices(df, column_name):\n",
        "    # Clean the column by removing any non-numeric characters except the decimal point\n",
        "    df['COST_Cleaned_Price'] = df[column_name].replace('[^\\d.]', '', regex=True)\n",
        "\n",
        "    # Convert the cleaned strings to numeric type (float)\n",
        "    df['COST_FINAL'] = pd.to_numeric(df['COST_Cleaned_Price'], errors='coerce')\n",
        "\n",
        "    # Create a new column to indicate the validity of the data\n",
        "    df['COST_Data_check'] = 'OK'  # Default to 'OK'\n",
        "    df.loc[df['COST_FINAL'].isna(), 'COST_Data_check'] = 'Check'  # Mark non-convertible or missing entries\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "\n",
        "# Apply the function\n",
        "df_10 = clean_and_check_prices(df_9, 'COST')\n",
        "df_10"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "ieK8nc3oqOSB",
        "outputId": "c31b3b99-186a-446a-af67-85d858a55120"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def verify_product(df):\n",
        "    # Calculate the product of HOURS and RATE\n",
        "    df['Calculated_COST'] = df['HOURS_FINAL'] * df['RATE_FINAL']\n",
        "\n",
        "    # Initialize the check column with 'OK' where the calculated product matches COST and handle NaNs\n",
        "    df['HOURS_RATE_COST_check'] = np.where(\n",
        "        (df['Calculated_COST'] == df['COST_FINAL']) &\n",
        "        df['HOURS_FINAL'].notna() & df['RATE_FINAL'].notna() & df['COST_FINAL'].notna(),\n",
        "        'OK',\n",
        "        'Check'\n",
        "    )\n",
        "\n",
        "    # Optionally, drop the 'Calculated_COST' column if it's not needed\n",
        "    df.drop(columns=['Calculated_COST'], inplace=True)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# Apply the function\n",
        "df_11 = verify_product(df_10)\n",
        "df_11\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZN6P35dVqOP_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZajagQrZqONv"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HhHmqhdPqOLw"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jth2t7Q0qOJQ"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ep6TcUzLY8Yc",
        "outputId": "393c5f66-f2e7-48d6-d9d7-bc736f7386c3"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "\n",
        "folder_path = '/content/data'  # Change this to your specific folder path\n",
        "\n",
        "# List all files in the directory\n",
        "files = os.listdir(folder_path)\n",
        "\n",
        "STATUS_list=[]\n",
        "DATE_list=[]\n",
        "SHIFT_list=[]\n",
        "HOURS_list=[]\n",
        "RATE_list=[]\n",
        "COST_list=[]\n",
        "ON_CALL_list=[]\n",
        "ROLE_list=[]\n",
        "AREA_list=[]\n",
        "\n",
        "# Loop over the list of files\n",
        "for file in files:\n",
        "    if file.endswith('.xlsx'):\n",
        "        file_path = os.path.join(folder_path, file)\n",
        "\n",
        "        values_to_check = ['VACANT', 'PENDING', 'NEW','CURRENT']\n",
        "        sheets_found = find_sheets_with_values(file_path, values_to_check)\n",
        "        try:\n",
        "          sheets_found.remove('Validation data')\n",
        "        except:\n",
        "          print(\"A\")\n",
        "\n",
        "        for i in sheets_found:\n",
        "          print(i)\n",
        "          print(\"---------------------\")\n",
        "          sheet_name = i\n",
        "          data_frame = load_sheet_to_df(file_path, sheet_name)\n",
        "\n",
        "\n",
        "          keywords = ['VACANT', 'PENDING', 'NEW','CURRENT']\n",
        "\n",
        "          row_index, col_name, col_index = find_first_keyword(data_frame, keywords)\n",
        "\n",
        "          row_index = row_index\n",
        "          col_index = col_index\n",
        "\n",
        "\n",
        "          new_df = extract_subset_df(data_frame, row_index, col_index)\n",
        "\n",
        "          new_new_df = extract_first_nine_columns(new_df)\n",
        "          final_df = remove_all_nan_rows(new_new_df)\n",
        "\n",
        "          new_column_names = [\"STATUS\", \"DATE\", \"SHIFT\", \"HOURS\", \"RATE\", \"COST\", \"ON_CALL\", \"ROLE\",\"AREA\"]\n",
        "\n",
        "          if len(new_column_names) == len(final_df.columns):\n",
        "              final_df.columns = new_column_names\n",
        "          else:\n",
        "              print(\"Error: The number of new column names does not match the number of existing columns.\")\n",
        "\n",
        "          # final_df['DATE'] = final_df['DATE'].apply(convert_date_column)\n",
        "\n",
        "          # last_df = split_and_format_time(final_df, 'SHIFT')\n",
        "\n",
        "          # new_df = final_df.drop(['SHIFT','From Time', 'To Time'], axis=1)\n",
        "\n",
        "          for  k in new_column_names:\n",
        "            try:\n",
        "              unique_list = final_df[k].tolist()\n",
        "              if k==\"STATUS\":\n",
        "                  STATUS_list.extend(unique_list)\n",
        "\n",
        "              if k==\"DATE\":\n",
        "                  DATE_list.extend(unique_list)\n",
        "\n",
        "              if k==\"SHIFT\":\n",
        "                  SHIFT_list.extend(unique_list)\n",
        "\n",
        "              if k==\"HOURS\":\n",
        "                  HOURS_list.extend(unique_list)\n",
        "\n",
        "              if k==\"RATE\":\n",
        "                  RATE_list.extend(unique_list)\n",
        "\n",
        "              if k==\"COST\":\n",
        "                  COST_list.extend(unique_list)\n",
        "\n",
        "              if k==\"ON_CALL\":\n",
        "                  ON_CALL_list.extend(unique_list)\n",
        "\n",
        "              if k==\"ROLE\":\n",
        "                  ROLE_list.extend(unique_list)\n",
        "\n",
        "              if k==\"AREA\":\n",
        "                  AREA_list.extend(unique_list)\n",
        "\n",
        "            except:\n",
        "              print(\"error_at_sheet\",sheet_name)\n",
        "              print(\"file_name\",file_path)\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AmyrwtrUwNDE"
      },
      "outputs": [],
      "source": [
        "STATUS_list=[]\n",
        "DATE_list=[]\n",
        "SHIFT_list=[]\n",
        "HOURS_list=[]\n",
        "RATE_list=[]\n",
        "COST_list=[]\n",
        "ON_CALL_list=[]\n",
        "ROLE_list=[]\n",
        "AREA_list=[]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Iyqbfq0oaNoI",
        "outputId": "91c49c16-6629-403a-975d-c015fe5c24d1"
      },
      "outputs": [],
      "source": [
        "STATUS_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6kXFXffaaeAz",
        "outputId": "74cc9c58-1015-47e4-f9d3-136dad6776be"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "def count_unique_values(input_list):\n",
        "    # Use Counter to count occurrences of each element\n",
        "    count = Counter(input_list)\n",
        "\n",
        "    # Print each unique value and its count\n",
        "    for item, frequency in count.items():\n",
        "        print(item,\":\" ,frequency)\n",
        "\n",
        "\n",
        "my_list = [1, 2, 2, 3, 4, 4, 4, 5, 6, 6, 7]\n",
        "count_unique_values(my_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nyUdJBiPqI8g",
        "outputId": "8c9b732e-c5ad-4978-e943-8d61ccd56025"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(STATUS_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(STATUS_list))\n",
        "# if not pending vacant new after making all capital show monitor.\n",
        "#if empy need monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQA2v126ukDn",
        "outputId": "4b5ed62f-5808-4211-dcd0-2cc37172cdd9"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(DATE_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(DATE_list))\n",
        "\n",
        "# make it in to data format if not valid and if it's empty need monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J1UgitySukBj",
        "outputId": "f586b40b-82fb-435b-d8bd-44bba0dc45db"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(SHIFT_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(SHIFT_list))\n",
        "\n",
        "#if not in 0000-0000 format need monitor\n",
        "#if any text inside need monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oXeOcCa3zv16",
        "outputId": "cf255db7-6d87-4dc5-accc-694059727e19"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(HOURS_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(HOURS_list))\n",
        "\n",
        "#not a number need monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QxPhhukYzxAi",
        "outputId": "15e3b12b-665b-477a-c936-4464ddf02fd7"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(RATE_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(RATE_list))\n",
        "\n",
        "#if 24 hours in it remove it and only take number\n",
        "# If text there need monitor\n",
        "#if nan monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o8tF6mddzxc0",
        "outputId": "4b5bd7ce-a4f7-434b-9a80-4f3ab37c057a"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(COST_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(COST_list))\n",
        "\n",
        "#if 24 hours in it remove it and only take number\n",
        "# If text there need monitor\n",
        "#if nan monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1eqhNWQ7zx0G",
        "outputId": "f184c7bf-2f47-45a5-dc41-084b8c99b173"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(ON_CALL_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(ON_CALL_list))\n",
        "\n",
        "#if anything otehr than YES or NO need monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jxuovm2bzyHp",
        "outputId": "c02da166-5050-449f-8187-941554c4d46f"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(ROLE_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(ROLE_list))\n",
        "\n",
        "# if have '1700-0859' like dara need monitor, also if we have Yes or NO dta need monitor\n",
        "# if nan we need monitor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kg3NertfWQYl"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nzQV82euz-15",
        "outputId": "d32dd5d8-d1b9-4a5d-af9d-795ea81ce881"
      },
      "outputs": [],
      "source": [
        "\n",
        "unique_list = list(dict.fromkeys(AREA_list))\n",
        "print(unique_list)\n",
        "print(count_unique_values(AREA_list))\n",
        "\n",
        "# if have '1700-0859' like dara need monitor, also if we have Yes or NO dta need monitor\n",
        "# if nan we need monitor"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
